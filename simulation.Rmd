---
title: "Simulation"
output: github_document
    
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(rvest)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Simulate data

Make fake random data on a computer that behaves like real data, so you can see what would happen if you collected samples many times.

# Step 1

```{r}
library(tidyverse)
set.seed(1)
```

explanation:
- `tidyverse` = loads helpful tools (`dplyr`, `ggplot2`, etc.)
- `set.seed(1)` = ensures your random numbers are the same each time (for reproducibility)

# Step 2
Write a simulation function for mean and sd.

```{r}
sim_mean_sd = function(n, mu=2, sigma=3){
  
  sim_data = tibble(
    x = rnorm(n, mean=mu)
  )
  
  sim_data |> 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}

sim_mean_sd(30)
```
explanation:
 - `n` = no. of data points (sample size)
 - `mu` = true pop. mean.
 - `sigma` = true pop sd.
 - `rnorm()` = randomly generate `n` no. from a normal distribution
 - `summarize()` = calculate the sample's mean (mu_hat) and sd(sigma_hat).
 - return a small table (tibble) with those two numbers
 - test with 30
 
## Steo 3: Run the simulation 100 times
```{r}

output = vector("list", 100)

for (i in 1:100){
  output[[i]] = sim_mean_sd(30)
}

sim_results = bind_rows(output)

```
explanation:
- `vector("list", 100)` makes an empty list with 100 slots.
- the `for` loops repeats 100 times.
- each time, it runs `sim_mean_sd(30)` (30 observations per sample).
- `output[[i]]` saves the result of each run.
- `bind_rows()` stacks them all together into one big data frame.
--> Now you have 100 estimates of mean (mu_hat) and sd (sigma_hat).

## Step 4: Do the same thing using map()(Simpler)
instead of the for-loop:
```{r}
sim_results_df=
  expand_grid(
    sample_size = 30,
    iter = 1:100
  ) |> 
  mutate(
    estimate_df = map(sample_size, sim_mean_sd)
  ) |> 
  unnest(estimate_df)

```
explanation:
 - `expan_grind()` makes a mini data frame with 100 rows (`iter` 1-100, `sample_size`= 30 each).
 - `mutate()` adds a new column called `estimate_df`.
 - `map()` applies the function `sim_mean_sd` to each row's `sample_size`.
 - `unnest()` pulls the results (means and sds) out of the list column into normal columns.
 
## Step 5: Visualize and summarize results.
Plot the distribution of simulated means:
```{r}
sim_results_df |> 
  ggplot(aes(x=mu_hat))+
  geom_density()
```
it shows:
- the spread of the 100 estimated means (mu_hat).
- it looks roughly normal (bell-shapped) centered around 2 (the true mean).
 
Summarize numerically:
```{r}
sim_results_df |> 
  pivot_longer (mu_hat:sigma_hat, names_to = "parameter", values_to = "estimate") |> 
  group_by(parameter) |> 
  summarize(
    emp_mean = mean(estimate),
    emp_sd = sd(estimate)
  ) |> 
  knitr::kable(digits=3)
```
explanation:
- `pivot_longer()` stacks both columns (mu_hat, sigma_hat) into one for easier grouping.
- `group_by(parameter)` separates results for mean and sd.
- `summarize()` computes the average of all simulated estimates and their spread.

## Step 6: Shorthande with anonymous function

```{r}
sim_results_df=
  map(1:100, \(i) sim_mean_sd(30,2,3)) |>
  bind_rows()
```
explanation:
- `map(1:100, \(i)...)` = repeats something 100 times.
- the `\(i)` part just defines a temporary variable `i` that isn't used.
- inside, we call `sim_mean_sd(30,2,3)` each time.
- `bind_rows()` combines the 100 results.
- same outcome, just faster to write. 

## Step 7: Simulate for several sample sizes.
We now test how increasing `n` affects results.
```{r}
sim_results_df=
  expand_grid(
    sample_size = c(30, 60, 120, 240),
    iter = 1:500
  ) |> 
  mutate(
    estimate_df = map(sample_size, sim_mean_sd)
  ) |> 
  unnest(estimate_df)
```
explanation:
- runs 500 simulations for each of 4 sample sizes (30, 60, 120, 240).
- stores all results together in one big data frame.

## Step 8: Compate results across sample sizes
Plot
```{r}
sim_results_df |> 
  mutate(
    sample_size = str_c("n= ", sample_size),
    sample_size = fct_inorder(sample_size)
  ) |> 
  ggplot(aes(x= sample_size, y=mu_hat, fill = sample_size)) +
  geom_violin()
```
- each "violin" shows the spread of the estimated means for one sample size.
- wider = more variation.
- as `n` increases, the violin gets thinner = estimates get more stable.

Numeric summary:
```{r}
sim_results_df |> 
  pivot_longer(mu_hat:sigma_hat, names_to = "parameter", values_to = "estimate") |> 
  group_by(parameter, sample_size) |> 
  summarize(
    emp_mean = mean(estimate),
    emp_var = var(estimate)
  ) |> 
  knitr::kable(digits=3)
```

## Step 9: Stimulate regression (SLR)
Now we simulate a `linear regression` model.

```{r}
sim_regression = function(n, beta0 = 2, beta1 = 3){
  sim_data = tibble(
    x = rnorm(n, mean=1, sd=1),
    y = beta0 + beta1*x + rnorm(n, 0,1)
  )
  
  ls_fit = lm(y~x, data = sim_data)
  
  tibble(
    beta0_hat = coef(ls_fit)[1],
    beta1_hat = coef(ls_fit)[2]
  )
}
```
explanation:
 - creates `n` random x-values.
 - creates `y` values using the formula `y=beta0 + beta1*x + error`.
 - fits a linear model `lm()`.
 - returns the estimated slope and intercept.
 
## Step 10: Repeat regression 500 times

```{r}
sim_results_df=
  expand_grid(
    sample_size =30,
    iter = 1:500
  ) |> 
  mutate(
    estimate_df = map(sample_size, sim_regression)
  ) |> 
  unnest(estimate_df)
```

## Step 11: Visualize regression results

```{r}
sim_results_df |> 
  ggplot(aes(x=beta0_hat, y=beta1_hat))+
  geom_point()
```
- each dot = one simulated regression,
- the pattern shows that intercept (beta0) and slope (beta1) estimates are negatively correlated (when one's higher, the other's lower).

## Step 12: Vary two parameters (sample size + true sd)
Now we stimulate across combinations of sample size and true sd.
```{r}
sim_results_df=
  expand_grid(
    sample_size = c(30, 60, 120, 240),
    true_sd = c(6,3),
    iter = 1:500
  ) |> 
  mutate(
    estimate_df = 
      map2(sample_size, true_sd, \(n, sd) sim_mean_sd(n=n, sigma=sd))
  ) |> 
  unnest(estimate_df)
```
explanation:
- `expand_grid()` makes all combinations of `n` and `true_sd`.
- `map2()` lets you pass two changing inputs into your function.
- each run uses a different combination of `sample_size` and `sigma`.

## Step 13: Visualize results (two-parameter simulation)
```{r}
sim_results_df |> 
  mutate(
    true_sd = str_c("True sd:", true_sd),
    true_sd = fct_inorder(true_sd),
    sample_size = str_c("n=", sample_size),
    sample_size = fct_inorder(sample_size)
  ) |> 
  ggplot(aes(x=sample_size, y=mu_hat, fill=sample_size))+
  geom_violin()+
  facet_grid(. ~true_sd)
```
This shows:
- each facet = one true sd (3 or 6).
- within each facet, you see how the spread of the mean estimate shrinks as `n` grows.
- larger true sds produce wider distribution, more noise in the data.

## Important final summary:
- `sim_mean_sd()`: function to simulate random data and return mean/sd.
- `for` loop: repeats simulation many times.
- `map()`: cleaner way to repeat a function over many inputs.
- `expaned_grid()`: generates all combinations of simulation settings.
- `unnest()`: pulls results out of list columns into normal columns.
- `map2()`: lets you very two inputs at once.
- `geom_violin()`: visualizes how distributions change.






















